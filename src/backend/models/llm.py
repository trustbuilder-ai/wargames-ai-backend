"""Pydantic models for LLM interactions.

This module defines the data models used for LLM API requests and responses,
following the OpenAI ChatCompletions API format for consistency.
"""

from typing import Any, Literal, Union

from pydantic import BaseModel, Field


class ChatMessage(BaseModel):
    """A single chat message in the conversation.

    Attributes:
        role: The role of the message sender (user, assistant, system).
        content: The content of the message.
        name: Optional name of the message sender.
    """

    role: Literal["user", "assistant", "system"]
    content: str
    name: str | None = None


class ChatRequest(BaseModel):
    """Request model for chat completions.

    Attributes:
        model: The model to use for completion.
        messages: List of messages in the conversation.
        temperature: Sampling temperature (0.0 to 2.0).
        max_tokens: Maximum tokens in the response.
        stream: Whether to stream the response.
        user: Optional user identifier for tracking.
    """

    model: str
    messages: list[ChatMessage]
    temperature: float | None = Field(default=0.7, ge=0.0, le=2.0)
    max_tokens: int | None = Field(default=4096, ge=1, le=8192)
    stream: bool | None = False
    user: str | None = None


class ChatChoice(BaseModel):
    """A single choice in the chat completion response.

    Attributes:
        index: The index of this choice.
        message: The message generated by the model.
        finish_reason: The reason the model stopped generating.
    """

    index: int
    message: ChatMessage
    finish_reason: str | None


class ChatUsage(BaseModel):
    """Token usage information for the chat completion.

    Attributes:
        prompt_tokens: Number of tokens in the prompt.
        completion_tokens: Number of tokens in the completion.
        total_tokens: Total number of tokens used.
    """

    prompt_tokens: int
    completion_tokens: int
    total_tokens: int


class ChatResponse(BaseModel):
    """Response model for chat completions.

    Attributes:
        id: Unique identifier for the completion.
        object: The object type (always "chat.completion").
        created: Unix timestamp of when the completion was created.
        model: The model used for the completion.
        choices: List of completion choices.
        usage: Token usage information.
    """

    id: str
    object: str
    created: int
    model: str
    choices: list[ChatChoice]
    usage: ChatUsage | None = None


class ModelInfo(BaseModel):
    """Information about an available model.

    Attributes:
        id: The model identifier.
        object: The object type (always "model").
        created: Unix timestamp of when the model was created.
        owned_by: The organization that owns the model.
        display_name: Human-readable name for UI display.
        description: Model description.
        max_tokens: Maximum tokens supported.
    """

    id: str
    object: str = "model"
    created: int
    owned_by: str
    display_name: str | None = None
    description: str | None = None
    max_tokens: int | None = None


class ModelsResponse(BaseModel):
    """Response model for listing available models.

    Attributes:
        object: The object type (always "list").
        data: List of available models.
    """

    object: str = "list"
    data: list[ModelInfo]


class LLMHealthStatus(BaseModel):
    """Health status response for LLM service.

    Attributes:
        status: Health status (healthy, unhealthy, degraded).
        available_models: Number of models with valid API keys.
        missing_keys: List of missing API key environment variables.
        message: Human-readable status message.
        details: Additional status details.
    """

    status: Literal["healthy", "unhealthy", "degraded"]
    available_models: int
    missing_keys: list[str]
    message: str
    details: dict[str, Any] | None = None


class ErrorDetail(BaseModel):
    """Error detail information.

    Attributes:
        message: Human-readable error message.
        type: Error type identifier.
        param: Parameter that caused the error, if applicable.
        code: Error code, if applicable.
    """

    message: str
    type: str
    param: str | None = None
    code: str | None = None


class ErrorResponse(BaseModel):
    """Error response model.

    Attributes:
        error: Error details.
    """

    error: ErrorDetail


# ============================================================================
# Tool-related models for agentic capabilities
# ============================================================================


class ToolFunction(BaseModel):
    """Function definition for tool calling.

    Attributes:
        name: The name of the function to be called.
        description: Description of what the function does.
        parameters: JSON Schema for the function parameters.
    """

    name: str
    description: str
    parameters: dict[str, Any] = Field(
        default_factory=lambda: {"type": "object", "properties": {}}
    )


class FunctionCall(BaseModel):
    """Function call details within a tool call.

    Attributes:
        name: The name of the function to call.
        arguments: The arguments to call the function with (as JSON string).
    """

    name: str
    arguments: str  # JSON string of arguments


class ToolCall(BaseModel):
    """A tool call request from the model.

    Attributes:
        id: Unique identifier for this tool call.
        type: The type of tool call (currently only 'function').
        function: The function call details.
    """

    id: str
    type: Literal["function"] = "function"
    function: FunctionCall


class ChatMessageWithTools(ChatMessage):
    """Extended chat message that can include tool calls.

    Attributes:
        tool_calls: List of tool calls requested by the assistant.
        tool_call_id: ID of the tool call this message is responding to.
    """

    role: Literal["user", "assistant", "system", "tool"] # type: ignore
    tool_calls: list[ToolCall] | None = None
    tool_call_id: str | None = None


class ChatRequestWithTools(ChatRequest):
    """Extended chat request that can include tool definitions.

    Attributes:
        tools: List of available tools for the model to use.
        tool_choice: How the model should choose tools ('auto', 'none', or specific).
    """

    messages: list[ChatMessageWithTools] # type: ignore
    tools: list[ToolFunction] | None = None
    tool_choice: Union[Literal["auto", "none"], dict[str, str]] | None = None


class ChatChoiceWithTools(ChatChoice):
    """Extended chat choice that can include tool calls.

    Attributes:
        message: The message with potential tool calls.
    """

    message: ChatMessageWithTools # type: ignore


class ChatResponseWithTools(ChatResponse):
    """Extended chat response that can include tool calls.

    Attributes:
        choices: List of completion choices with tool support.
    """

    choices: list[ChatChoiceWithTools] # type: ignore


class ToolExecutionResult(BaseModel):
    """Result from executing a tool.

    Attributes:
        tool_call_id: ID of the tool call this is responding to.
        output: The output from the tool execution.
        error: Error message if the tool execution failed.
    """

    tool_call_id: str
    output: str | None = None
    error: str | None = None


class ConversationTurn(BaseModel):
    """A single turn in a tool-calling conversation.

    Attributes:
        request: The request sent to the LLM.
        response: The response from the LLM.
        tool_results: Results from any tool executions.
    """

    request: ChatRequestWithTools
    response: ChatResponseWithTools
    tool_results: list[ToolExecutionResult] | None = None



ChatEntry = ChatRequest | ChatResponse | ChatRequestWithTools | ChatResponseWithTools
